{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Rigor\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "In medicine, just as in surgical procedures, the key to success lies in precision and attention to detail. That's what 'rigor' stands for in clinical research. When we talk about rigor, we're referring to the thoroughness and precision in research that makes our findings reliable and actionable. Think of it as the preparations a surgeon would do before an operation. We want our research to be set up perfectly, so the results truly benefit patient care.\n",
    "\n",
    "![](https://github.com/jhutrc/beta/blob/main/act_0/act_0_2/act_0_2_1/fig1_cisplatin.png?raw=true)\n",
    "\n",
    "**Understanding the Clinical Problem**\n",
    "\n",
    "Every research journey begins with a question. This could be about a specific treatment's effectiveness, or understanding a disease pattern. To tackle this, we first need a crystal clear definition of our problem. It’s much like a surgeon needing a detailed diagnosis before determining the best surgical approach.\n",
    "\n",
    "**Designing the Study and Gathering Data**\n",
    "\n",
    "Designing a study is akin to mapping out a surgical strategy. We need the right 'patients' (participants), dependable methods to gather our data, and to ensure our data is consistent and spot-on. Any misstep can blur our findings, similar to how a surgical procedure can go awry without a proper plan.\n",
    "\n",
    "**Analysis and Sharing Results**\n",
    "\n",
    "Once our data is ready, it's interpretation time. Imagine a post-surgery review, evaluating if all went as planned. Here's where terms like 'odds' come in. Let’s gently unpack this:\n",
    "\n",
    "**The Image Explained**: Imagine a group of patients with bladder cancer. For each patient, there's a decision to make - are they suitable for a specific treatment called cisplatin? It's crucial because this treatment, while potentially beneficial, can also have adverse outcomes.\n",
    "\n",
    "We have two tools, or methods, to help make this decision: CKD-EPI 2021 (a newer, race-free equation) and CKD-EPI 2009. These methods give us \"odds\", essentially the chances, of a patient being eligible for the treatment (Cisplatin+) or not (Cisplatin-). But there's more: these odds also hint at the likelihood of a patient facing adverse outcomes if they receive the treatment.\n",
    "\n",
    "Now, you might wonder, which method do we trust? To figure that out, we compare the odds given by both methods. This comparison, known as the 'odds ratio', is our compass. It shows us how the odds from one method stack up against the other. And because in medicine we always deal with uncertainties, we also present a range (the 95% CI) to show where the true odds ratio likely lies. Think of it as the margin of error in our compass, helping us navigate the decision-making journey more confidently.\n",
    "\n",
    "In essence, the image serves as a visual guide, simplifying complex decisions and potential outcomes into understandable terms.\n",
    "\n",
    "Being transparent in how we reached our results is as crucial as a surgeon discussing outcomes with a patient's family. It builds trust and allows others to understand, replicate, and expand on our work.\n",
    "\n",
    "**In Conclusion**\n",
    "\n",
    "Rigor in research is all about detail and reliability. From framing the right questions to interpreting and communicating findings, it's a journey to provide the best possible answers for better patient care. \n",
    "\n",
    "<Details>\n",
    "\n",
    "  <Summary>Analytic Design</Summary>\n",
    "\n",
    "1. **Eligibility: Study Design (Figure 1)**: \n",
    "\n",
    "- **Factorial Design**: An example is given above on cisplatin therapy in patients with bladder cancer.\n",
    "\n",
    "2. **Binary: Study vs. Control (Table 1)**\n",
    "\n",
    "- **Propensity for Outcomes**: Similar propensity for outcomes (*ceteris paribas*) between the study and control group. If the study is randomized, the propensity for outcomes should theoretically be balanced across groups. If not, you might consider using statistical techniques like propensity score matching to balance observed confounders. Multivariable regression doesn't ensure balance between groups, so it's not a substitute for propensity score matching.\n",
    "\n",
    "3. **Continuous: Difference in Means (Figure 2)**\n",
    "\n",
    "- **Surrogate of Outcome of Interest**: It's important that the continuous measure is a valid surrogate for the ultimate outcome of interest. Its validity and reliability should be well-established.\n",
    "  \n",
    "   + **Longitudinal**: Repeated measures of the same individuals over time can be powerful but require specialized statistical models like mixed-effects models or generalized estimating equations to account for within-subject correlation.\n",
    "\n",
    "   + **Hierarchical**: If data are clustered by some phenotype or other categorization (like hospital or geographic location), you'll need to use hierarchical or multilevel models to account for this clustering. This is particularly important for avoiding Type I errors (false positives).\n",
    "\n",
    "4. **Time-to-event: Survival Analysis (Figure 3)**\n",
    "\n",
    "- **Hard Clinical Outcomes with ICD10 Code**: Using ICD10 codes for outcomes is a strong approach as these are standardized diagnoses. However, ensure the data quality and consistency in how these codes are assigned.\n",
    "  \n",
    "5. **Interaction Effects**: You might also consider if there are any interaction effects between these different types of data (e.g., does the impact of a treatment vary over time?).\n",
    "\n",
    "6. **Multiple Comparisons**: If you're looking at multiple outcomes or conducting numerous subgroup analyses, you'll need to adjust for multiple comparisons to maintain the study’s Type I error rate.\n",
    "\n",
    "7. **Missing Data**: Plan how to handle missing data, as this can significantly impact your results. Note that \"not handling missing data\" is equivalent to complete case analysis (i.e., only including individuals with complete data). This can introduce bias if the missing data are not missing completely at random (MCAR). Multiple imputation is a common approach to handling missing data. If data are missing completely at random (MCAR), then complete case analysis is a valid approach but resulting in a loss of power.\n",
    "\n",
    "8. **Data Visualization**: A well-designed table or figure can be worth a thousand words but be cautious of over-interpretation. Make sure the visualization clearly conveys what the data represent.\n",
    "\n",
    "9. **External Validity**: Finally, consider how generalizable your findings are to the broader population, a historical population, or a future one (which is often the target).\n",
    "\n",
    "</Details>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
